{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3748c3-1ed6-4870-876b-81aac7ed4c8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TimeSync Image Chip and Spectral Value Time Series Extraction Script\n",
    "## This workflow demonstrates how to extract Landsat C2 ARD image and spectral data time series in AWS over reference plots using CHS Pangeo.\n",
    "### Last Updated: 07-28-2023\n",
    "#### Authors: Cole Krehbiel and Kelcy Smith (original), D. Wellington (revised)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbdad94-7b28-4b41-ae5e-f12df3bd9027",
   "metadata": {},
   "source": [
    "The TimeSync software tool relies on pre-formatted image and spectral data loaded onto the appropriate location on the TimeSync server. This notebook generates these data files for subsequent download to the TimeSync server.\n",
    "\n",
    "The data that must be generated for each TimeSync project are as follows:\n",
    "\n",
    "1) A set of CSV files that contains the sensor ID, project ID, plot ID, tile ID, year, day-of-year, datetime, and individual pixel values derived from the blue, green, red, NIR, SWIR1, SWIR2, and QA bands. The QA value is represented as either 0 (clear) or 1 (cloudy). These csv files are output individually for each observation and combined in subsequent processing.\n",
    "\n",
    "2) Three image composites, output as three-channel, 8 bits-per-channel PNGs, as follows:\n",
    "      * A composite of NIR/red/green bands (equivalent to TM/ETM+ bands 4/3/2)\n",
    "      * A composite of SWIR2/NIR/red bands (equivalent to TM/ETM+ bands 7/4/3)\n",
    "      * A composite of tasseled cap brightness/greenness/wetness bands\n",
    "\n",
    "Plots with fill at the plot pixel location are excluded from both csv and image chip generation. Fill outside the center pixel location in image chips is allowed.\n",
    "\n",
    "The three composites (or \"image chips\") cover an area of 255x255 pixels, centered on the plot location. These locations must be provided in a CSV file with coordinates specified in the Landsat ARD coordinate grid. In all composites, fill values are set to black (0/0/0) in the output PNG."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29fc269-ec03-4fd7-82ae-9276f7a42450",
   "metadata": {},
   "source": [
    "## Instructions for Use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa0a884-0bf0-4b06-8d0e-1c1764c800ff",
   "metadata": {},
   "source": [
    "### 1. Update the parameters dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99290444-7970-48d0-a77b-986c508340b1",
   "metadata": {},
   "source": [
    "The following parameter dictionary MUST be updated before running. The parameters are as follows:\n",
    "- **project_dir**: The s3 bucket location and path-like prefix for objects created in this project. Make sure this is different for every project.\n",
    "- **plot_file**: A csv file that contains these headers: project_id, plot_id, x, y. This is assumed to be a local file. Use the file browser in the JuypterLab environment to upload the file, and specify the path using this parameter.\n",
    "- **region**: Only 'CU' (for CONUS) is accepted by default, but other regions could be enabled by adding them to the `tile_grid_affine()` function\n",
    "- **chip_size**: For default TimeSync, leave this as [255, 255] (width, height in pixels)\n",
    "- **process_on**: Options are 'kube_cluster' or 'local'. The 'local' option is for debugging, you generally want 'kube_cluster'. Subsequent instructions assume you are operating on the cluster; otherwise, skip step 3 and do not try to pass a client object at execution.\n",
    "- **store_on**: Options are 'aws_s3' or 'local'. The 'local' option is for debugging, you generally want 'aws_s3'.\n",
    "- **profile**: This must be a the name of an AWS profile with write access to the project_dir bucket, if writing to s3 (if not writing to an s3 bucket, this parameter is optional and ignored). Profiles with access credentials are listed in ~/.aws/credentials. if you have never written to an AWS S3 bucket, you may need to request access from CHS and follow AWS configuration instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f029d8-c363-4c44-bb91-64e00e0dab45",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "Update the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0aabbd-e8c9-40ac-b58b-fa7c1eefe17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'project_dir': 's3://dev-nlcd-developer/junk/timesync/', \n",
    "    'plot_file': './10lines_PlotList.csv', \n",
    "    'region': 'CU', \n",
    "    'chip_size': [255, 255],\n",
    "    'process_on': 'local', \n",
    "    'store_on': 'aws_s3', \n",
    "    'profile': 'default',\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296433e8-e714-4f50-9a5d-5c59e1223b70",
   "metadata": {},
   "source": [
    "### some of this AWS authentication stuff can be greatly simplified with %env or os.environment \n",
    "- like the requester pays bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca67668f-69e8-4ab4-8b50-4d2a70a82061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AWS_REQUEST_PAYER=requester\n"
     ]
    }
   ],
   "source": [
    "%env AWS_REQUEST_PAYER=requester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dabc800-3245-4f8c-b0e0-ab8dd8e58ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! aws s3 ls | grep dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91886244-db79-4452-aac1-8443c400fc7f",
   "metadata": {},
   "source": [
    "### 2. Import libraries and define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed80c1ba-0503-487e-a83d-0c0fb31797e1",
   "metadata": {},
   "source": [
    "Run the following cell, which contains all library imports and locally defined functions for data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e19bac61-68f9-4c8f-a14e-858d2b48f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import configparser\n",
    "from copy import copy\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime as dt\n",
    "from functools import partial, reduce, wraps\n",
    "from typing import List, Tuple, Optional, Any, Callable, Iterable\n",
    "\n",
    "import s3fs\n",
    "import tqdm\n",
    "import boto3\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pystac_client\n",
    "import rasterio as rio\n",
    "from dask.distributed import as_completed, worker_client, Client\n",
    "from dask.distributed.client import Future\n",
    "from fsspec.implementations.local import LocalFileSystem\n",
    "\n",
    "Affine = Tuple[float, float, float, float, float, float]\n",
    "\n",
    "\n",
    "# Constants\n",
    "QA_FILL = 0\n",
    "QA_CLEAR = 6\n",
    "QA_WATER = 7\n",
    "CONCURRENT_STAC_QUERIES = 2  # Prevents workers from consuming too much memory\n",
    "LANDSAT_ARD_C2_FILL_VALUE = 0\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Bounds:\n",
    "    \"\"\"\n",
    "    Class to hold spatial coordinate bounds\n",
    "    \"\"\"\n",
    "    min_x: float\n",
    "    max_x: float\n",
    "    min_y: float\n",
    "    max_y: float\n",
    "\n",
    "\n",
    "class StacRecord:\n",
    "    \"\"\"\n",
    "    Class with methods for parsing STAC records\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def sensor(record: dict) -> str:\n",
    "        \"\"\"\n",
    "        Parse the sensor shorthand abbreviation at the beginning of the observation id\n",
    "        \"\"\"\n",
    "        return record['id'].split('_')[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def hv(record: dict) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve the tile coordinates as a string patterned like hhvv\n",
    "        \"\"\"\n",
    "        return record['properties']['landsat:grid_horizontal'] + record['properties']['landsat:grid_vertical']\n",
    "\n",
    "    @staticmethod\n",
    "    def date(record: dict) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve the datetime string\n",
    "        \"\"\"\n",
    "        return record['properties']['datetime']\n",
    "\n",
    "    @staticmethod\n",
    "    def year(record: dict) -> int:\n",
    "        \"\"\"\n",
    "        Retrieve the observation year\n",
    "        Note: Do not try to use '%Y-%m-%dT%H:%M:%S.%fZ' as the format code, it will not work\n",
    "            because the microseconds are not always six digits\n",
    "        \"\"\"\n",
    "        date = record['properties']['datetime']\n",
    "        return dt.strptime(date.split('.')[0], '%Y-%m-%dT%H:%M:%S').year\n",
    "\n",
    "    @staticmethod\n",
    "    def doy(record: dict) -> int:\n",
    "        \"\"\"\n",
    "        Retrieve the observation DOY\n",
    "        Note: Do not try to use '%Y-%m-%dT%H:%M:%S.%fZ' as the format code, it will not work\n",
    "            because the microseconds are not always six digits\n",
    "        \"\"\"\n",
    "        date = record['properties']['datetime']\n",
    "        return dt.strptime(date.split('.')[0], '%Y-%m-%dT%H:%M:%S').timetuple().tm_yday\n",
    "\n",
    "    @staticmethod\n",
    "    def year_doy(record: dict) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve a formatted year/doy string\n",
    "        \"\"\"\n",
    "        return f'{StacRecord.year(record)}_{StacRecord.doy(record):03}'\n",
    "\n",
    "    @staticmethod\n",
    "    def asset_href(record: dict, band: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve the s3 location of a STAC asset\n",
    "        \"\"\"\n",
    "        return record['assets'][band]['alternate']['s3']['href']\n",
    "\n",
    "    @staticmethod\n",
    "    def crs(record: dict) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve the coordinate reference system from a STAC record as WKT\n",
    "        \"\"\"\n",
    "        return record['properties']['proj:wkt2']\n",
    "    \n",
    "    \n",
    "def retry(retries: int, jitter: Tuple[int, int] = (1, 15)) -> Callable:\n",
    "    \"\"\"\n",
    "    Simple retry decorator, for retrying any function that may throw an exception\n",
    "    such as when trying to retrieve network resources\n",
    "    \"\"\"\n",
    "    def retry_dec(func: Callable) -> Callable:\n",
    "        def wrapper(*args, **kwargs):\n",
    "            count = 1\n",
    "            while True:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception:\n",
    "                    count += 1\n",
    "                    if count > retries:\n",
    "                        raise\n",
    "                    time.sleep(random.randint(*jitter))\n",
    "        return wrapper\n",
    "    return retry_dec\n",
    "\n",
    "\n",
    "def timesync_band_list() -> List[str]:\n",
    "    \"\"\"\n",
    "    Get the bands relevant to TimeSync data retrieval\n",
    "    \"\"\"\n",
    "    return ['blue', 'green', 'red', 'nir08', 'swir16', 'swir22', 'qa_pixel']\n",
    "\n",
    "\n",
    "def landsat_optical_band_list() -> List[str]:\n",
    "    \"\"\"\n",
    "    Get the optical wavelength Landsat bands\n",
    "    \"\"\"\n",
    "    return ['blue', 'green', 'red', 'nir08', 'swir16', 'swir22']\n",
    "\n",
    "\n",
    "def centered_window(x: float, y: float, width: int, height: int, ds: rio.io.DatasetReader) -> rio.windows.Window:\n",
    "    \"\"\"\n",
    "    Create a window centered on the x, y of a pixel of interest\n",
    "    Width and height are in pixels\n",
    "    \"\"\"\n",
    "    row_offset, col_offset = ds.index(x, y)\n",
    "    return rio.windows.Window(\n",
    "        col_offset - (width // 2),\n",
    "        row_offset - (height // 2),\n",
    "        width,\n",
    "        height)\n",
    "\n",
    "\n",
    "def centered_bounds(x: float, y: float, width: int, height: int, pixel_size: int = 30):\n",
    "    \"\"\"\n",
    "    Create coordinate bounds centered on an x/y coordinate\n",
    "    \"\"\"\n",
    "    return Bounds(\n",
    "        min_x=x - ((width // 2) * pixel_size),\n",
    "        max_x=x + ((width // 2) * pixel_size),\n",
    "        min_y=y - ((height // 2) * pixel_size),\n",
    "        max_y=y + ((height // 2) * pixel_size))\n",
    "\n",
    "\n",
    "def single_pixel_window(x: float, y: float, ds: rio.io.DatasetReader) -> rio.windows.Window:\n",
    "    \"\"\"\n",
    "    Get a window representing a single pixel\n",
    "    \"\"\"\n",
    "    row_offset, col_offset = ds.index(x, y)\n",
    "    return rio.windows.Window(col_offset, row_offset, 1, 1)\n",
    "\n",
    "\n",
    "def build_query(h: int, v: int, region: str = 'CU', collection: str = 'landsat-c2ard-sr',\n",
    "                datetime: str = '1984-01/2022-12-31', limit: Optional[int] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Construct a STAC query based on h/v tile coordinates\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'collections': collection,\n",
    "        'datetime': datetime,\n",
    "        'limit': limit,\n",
    "        'query': {'landsat:grid_horizontal': {'eq': f'{h:02}'},\n",
    "                  'landsat:grid_vertical': {'eq': f'{v:02}'},\n",
    "                  'landsat:grid_region': {'eq': region}}}\n",
    "\n",
    "\n",
    "def tile_grid_affine(region: str) -> Affine:\n",
    "    \"\"\"\n",
    "    Get the ARD tile grid affine based on the regional code\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'CU': (-2565585, 150000, 0, 3314805, 0, -150000),  # CONUS\n",
    "    }[region]\n",
    "\n",
    "\n",
    "def transform_geo(x: float, y: float, affine: Affine) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Perform the affine transformation from an x/y coordinate to row/col space.\n",
    "    \"\"\"\n",
    "    col = (x - affine[0] - affine[3] * affine[2]) / affine[1]\n",
    "    row = (y - affine[3] - affine[0] * affine[4]) / affine[5]\n",
    "    return int(col), int(row)\n",
    "\n",
    "\n",
    "def determine_hv(x: float, y: float, region: str) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Determine the ARD tile (in h/v coordinates) containing the x/y coordinate\n",
    "    \"\"\"\n",
    "    h, v = transform_geo(x, y, tile_grid_affine(region))\n",
    "    return h, v\n",
    "\n",
    "\n",
    "def determine_hvs(bbox, region: str) -> itertools.product:\n",
    "    \"\"\"\n",
    "    Determine the h/v coordinates of tiles that intersect a bounding box\n",
    "    \"\"\"\n",
    "    min_h, min_v = determine_hv(bbox.min_x, bbox.max_y, region)\n",
    "    max_h, max_v = determine_hv(bbox.max_x, bbox.min_y, region)\n",
    "    return itertools.product(range(min_h, max_h + 1), range(min_v, max_v + 1))\n",
    "\n",
    "\n",
    "def query_stac(query_params: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Query the STAC catalog using the provided query parameters\n",
    "    \"\"\"\n",
    "    stac = pystac_client.Client.open('https://landsatlook.usgs.gov/stac-server')\n",
    "    # This returns a dictionary with two keys, 'type' and 'features'\n",
    "    results = stac.search(**query_params).item_collection_as_dict()\n",
    "    # 'type' only contains the value 'FeatureCollection'; we care about what is in 'features'\n",
    "    return results['features']\n",
    "\n",
    "\n",
    "def group_dicts(records: List[dict], key_func: Callable) -> itertools.groupby:\n",
    "    \"\"\"\n",
    "    Group a list of dictionaries based on key value\n",
    "    \"\"\"\n",
    "    records = sorted(records, key=key_func)\n",
    "    return itertools.groupby(records, key=key_func)\n",
    "\n",
    "\n",
    "def convert_sr(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Re-scale Landsat Collection 2 spectral data values back to the Collection 1 range\n",
    "    \"\"\"\n",
    "    return ((data.astype(float) * 0.0000275 - 0.2) * 10000).astype(np.int16)\n",
    "\n",
    "\n",
    "def data_to_collection_1(old_dict: dict, bands: List[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a dictionary of surface reflectance bands to the Landsat Collection 1 numerical range\n",
    "    \"\"\"\n",
    "    new_dict = old_dict.copy()\n",
    "    for key in bands:\n",
    "        new_dict[key] = convert_sr(old_dict[key])\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def read_bands(record: dict, bands: List[str], plot: Tuple[Any, ...], width: int, height: int) -> dict:\n",
    "    \"\"\"\n",
    "    Read in an ROI for an observation in the STAC record for all bands\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for band in bands:\n",
    "        with rio.open(StacRecord.asset_href(record, band)) as ds:\n",
    "            window = centered_window(plot.x, plot.y, width, height, ds)\n",
    "            # Get a masked array and fill it to avoid a bug with gdal/rasterio\n",
    "            out[band] = ds.read(1, window=window, boundless=True, fill_value=0, masked=True).filled()\n",
    "    return out\n",
    "\n",
    "\n",
    "def read_qa_at_plot(record: dict, plot: Tuple[Any, ...]) -> int:\n",
    "    \"\"\"\n",
    "    Read a single pixel at the plot location\n",
    "    \"\"\"\n",
    "    with rio.open(StacRecord.asset_href(record, 'qa_pixel')) as ds:\n",
    "        window = single_pixel_window(plot.x, plot.y, ds)\n",
    "        out = ds.read(1, window=window, boundless=False)\n",
    "    if out.size == 0:\n",
    "        return 1  # Treat values outside the spatial extent as fill\n",
    "    return out.item()\n",
    "\n",
    "\n",
    "def add_bands(dict_a: dict, dict_b: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Combine two observation dictionaries by adding the values for each band\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for band in dict_a:\n",
    "        out[band] = dict_a[band] + dict_b[band]\n",
    "    return out\n",
    "\n",
    "\n",
    "def composite(data: dict, bands: List[str], axis: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a multi-band ndarray from band names\n",
    "    \"\"\"\n",
    "    return np.stack([data[band] for band in bands], axis=axis)\n",
    "\n",
    "\n",
    "def tasseled_cap(data: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a composite of tasseled cap values\n",
    "    \"\"\"\n",
    "    band_order = ['blue', 'green', 'red', 'nir08', 'swir16', 'swir22']  # Must match coefficient order below\n",
    "    arr = np.stack([data[band] for band in band_order], axis=2)\n",
    "    b = np.tensordot(arr, [0.2043, 0.4158, 0.5524, 0.5741, 0.3124, 0.2303], axes=1)  # Brightness\n",
    "    g = np.tensordot(arr, [-0.1603, -0.2819, -0.4934, 0.7940, -0.0002, -0.1446], axes=1)  # Greenness\n",
    "    w = np.tensordot(arr, [0.0315, 0.2021, 0.3102, 0.1594, -0.6806, -0.6109], axes=1)  # Wetness\n",
    "    return np.stack([b, g, w])\n",
    "\n",
    "\n",
    "def build_affine(x_off: float, y_off: float, x_size: float = 30, y_size: float = 30, x_shear: float = 0,\n",
    "                 y_shear: float = 0) -> rio.Affine:\n",
    "    \"\"\"\n",
    "    Build the affine tuple in the rasterio format (different from GDAL)\n",
    "    \"\"\"\n",
    "    return rio.Affine(x_size, x_shear, x_off, y_shear, -y_size, y_off)\n",
    "\n",
    "\n",
    "def write_to_png(file: str, array: np.ndarray, crs: str, transform: rio.Affine) -> None:\n",
    "    \"\"\"\n",
    "    Write a PNG file as three 8-bit channels\n",
    "    \"\"\"\n",
    "    profile = {\n",
    "        'driver': 'PNG',\n",
    "        'count': 3,\n",
    "        'nodata': None,\n",
    "        'crs': crs,\n",
    "        'transform': transform,\n",
    "        'height': array.shape[1],\n",
    "        'width': array.shape[2],\n",
    "        'dtype': np.uint8}\n",
    "\n",
    "    with rio.open(file, mode='w', **profile) as ds:\n",
    "        ds.write(array)\n",
    "\n",
    "\n",
    "def array_mask(array: np.ndarray, value_to_mask = None, axis: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Boolean mask where the array matches the provided value anywhere along an axis\n",
    "    \"\"\"\n",
    "    return (array == value_to_mask).any(axis=axis)\n",
    "\n",
    "\n",
    "def apply_mask(array: np.ndarray, mask_array: np.ndarray, mask_value: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply a Boolean mask \n",
    "    \"\"\"\n",
    "    arr = array.copy()\n",
    "    arr[mask_array] = mask_value\n",
    "    return arr\n",
    "\n",
    "\n",
    "def byte_scale(array: np.ndarray, min_value: float, max_value: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Scale the data between min_value and max_value to 0-255\n",
    "    \"\"\"\n",
    "    out_array = (255 / (max_value - min_value)) * (array - min_value)\n",
    "    out_array = np.minimum(out_array, 255)\n",
    "    out_array = np.maximum(out_array, 0)\n",
    "    return out_array.astype(np.uint8)\n",
    "\n",
    "\n",
    "def byte_scale_bands(array: np.ndarray, all_bounds: List[Tuple[int, int]], \n",
    "                     mask: Optional[np.ndarray] = None, axis: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a multi-band array to scaled 8-bit\n",
    "    Masked values are set to 0\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i, (min_value, max_value) in enumerate(all_bounds):\n",
    "        byte_image = byte_scale(array.take(i, axis=axis), min_value, max_value)\n",
    "        out.append(apply_mask(byte_image, mask, mask_value=0))\n",
    "    return np.stack(out, axis=axis)\n",
    "\n",
    "\n",
    "def center(array: np.ndarray) -> Tuple[int, ...]:\n",
    "    \"\"\"\n",
    "    Get the indices for the center of an array\n",
    "    \"\"\"\n",
    "    return tuple(x // 2 for x in array.shape)\n",
    "\n",
    "\n",
    "def center_value(array: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Get the center value of an array\n",
    "    \"\"\"\n",
    "    return array[center(array)]\n",
    "\n",
    "\n",
    "def spectral_data(data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Get the data for the center pixel in the chip\n",
    "    \"\"\"\n",
    "    return {band: center_value(array) for band, array in data.items()}\n",
    "\n",
    "\n",
    "def df_to_csv(df: pd.DataFrame, params: dict, output: dict) -> None:\n",
    "    \"\"\"\n",
    "    Write the dataframe to the csv file\n",
    "    \"\"\"\n",
    "    with params['fs'].open(output['scsv'], 'w') as f:\n",
    "        df.to_csv(f, index=False)\n",
    "\n",
    "\n",
    "def classify_qa(qa: int) -> int:\n",
    "    \"\"\"\n",
    "    Return a value indicating the pixel is clear/water (0) or fill/cloud (1)\n",
    "    \"\"\"\n",
    "    if passes_qa_check(qa, enable_cloud_filtering=True):\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "def build_df(pixel_data: dict, record: dict, project_id: str, plot_id: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the output dataframe\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'sensor': StacRecord.sensor(record),\n",
    "        'project_id': project_id,\n",
    "        'plot_id': plot_id,\n",
    "        'hv': StacRecord.hv(record),\n",
    "        'year': StacRecord.year(record),\n",
    "        'doy': StacRecord.doy(record),\n",
    "        'blue': pixel_data['blue'],\n",
    "        'green': pixel_data['green'],\n",
    "        'red': pixel_data['red'],\n",
    "        'nir': pixel_data['nir08'],\n",
    "        'swir1': pixel_data['swir16'],\n",
    "        'swir2': pixel_data['swir22'],\n",
    "        'qa': classify_qa(pixel_data['qa_pixel']),\n",
    "        'date': StacRecord.date(record)}, index=[0])\n",
    "\n",
    "\n",
    "def invalid_pixel() -> dict:\n",
    "    \"\"\"\n",
    "    Get band values to represent and invalid pixel\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'blue': 0,\n",
    "        'green': 0,\n",
    "        'red': 0,\n",
    "        'nir08': 0,\n",
    "        'swir16': 0,\n",
    "        'swir22': 0,\n",
    "        'qa_pixel': 1}\n",
    "\n",
    "\n",
    "def adjust_for_s3(in_dict, filesystem) -> dict:\n",
    "    \"\"\"\n",
    "    Adjust the output file names to use the /vsis3/ file system handler for image data\n",
    "    \"\"\"\n",
    "    out_dict = copy(in_dict)\n",
    "    if isinstance(filesystem, s3fs.core.S3FileSystem):\n",
    "        for key in in_dict:\n",
    "            if in_dict[key].endswith('.png') or in_dict[key].endswith('.tif'):\n",
    "                out_dict[key] = in_dict[key].replace('s3:/', '/vsis3')\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def process_group(group: List[dict], plot: Tuple[Any, ...], params: dict) -> None:\n",
    "    \"\"\"\n",
    "    Process a group of STAC records associated with a plot into output PNGs\n",
    "    \"\"\"\n",
    "    with rio.Env(rio.session.AWSSession(boto3.Session(), requester_pays=True), \n",
    "                 GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR', \n",
    "                 GDAL_HTTP_MAX_RETRY=10,\n",
    "                 GDAL_HTTP_RETRY_DELAY=3):\n",
    "\n",
    "        # Set up the output filenames and (as applicable) directories\n",
    "        output = adjust_for_s3(\n",
    "            output_files(params['project_dir'], plot.project_id, plot.plot_id, StacRecord.year_doy(group[0])),\n",
    "            params['fs'])\n",
    "        for out in output.values():\n",
    "            make_dirs(params['fs'], out)\n",
    "\n",
    "        # Determine if the center pixel is fill\n",
    "        if not any(passes_qa_check(read_qa_at_plot(record, plot)) for record in group):\n",
    "            # Optionally, write an entry for an invalid pixel. This had been previous functionality,\n",
    "            # but because TimeSync does not expect this entry, I am disabling it.\n",
    "            # df_to_csv(build_df(invalid_pixel(), group[0], plot.project_id, plot.plot_id), params, output)\n",
    "            return\n",
    "\n",
    "        # Read and combine the data records\n",
    "        data_stack = [\n",
    "            read_bands(observation, timesync_band_list(), plot, params['chip_size'][0], params['chip_size'][1]) for\n",
    "            observation in group]\n",
    "        combined_data = data_to_collection_1(reduce(add_bands, data_stack), landsat_optical_band_list())\n",
    "\n",
    "    with rio.Env(**params['rio_env']):\n",
    "\n",
    "        # Get geospatial attributes\n",
    "        bounds = centered_bounds(plot.x, plot.y, params['chip_size'][0], params['chip_size'][1])\n",
    "        aff = build_affine(bounds.min_x, bounds.max_y)\n",
    "        crs = StacRecord.crs(group[0])\n",
    "        \n",
    "        # Mask for fill values\n",
    "        fill_value = convert_sr(np.array(LANDSAT_ARD_C2_FILL_VALUE))\n",
    "        mask = array_mask(composite(combined_data, landsat_optical_band_list()), fill_value)\n",
    "\n",
    "        # Calculate the output chip data\n",
    "        output_tcap = tasseled_cap(combined_data)\n",
    "        output_b743 = composite(combined_data, bands=['swir22', 'nir08', 'red'])\n",
    "        output_b432 = composite(combined_data, bands=['nir08', 'red', 'green'])\n",
    "\n",
    "        # Stretch the chip data and write to PNG\n",
    "        write_to_png(output['tcap'], byte_scale_bands(output_tcap, [(604, 5592), (49, 3147), (-2245, 843)], mask), crs, aff)\n",
    "        write_to_png(output['b743'], byte_scale_bands(output_b743, [(-904, 3696), (151, 4951), (-300, 2500)], mask), crs, aff)\n",
    "        write_to_png(output['b432'], byte_scale_bands(output_b432, [(151, 4951), (-300, 2500), (50, 1150)], mask), crs, aff)\n",
    "\n",
    "        # Define the metadata and spectral data for this observation and export to a csv\n",
    "        df_to_csv(build_df(spectral_data(combined_data), group[0], plot.project_id, plot.plot_id), params, output)\n",
    "\n",
    "\n",
    "def group_records(records: List[dict]) -> List[List[dict]]:\n",
    "    \"\"\"\n",
    "    Group records based on the observation ID\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for _, group in group_dicts(records, StacRecord.year_doy):\n",
    "        out.append(list(group))\n",
    "    return out\n",
    "\n",
    "\n",
    "def output_files(project_dir: str, project_id: str, plot_id: str, year_doy: str) -> dict:\n",
    "    \"\"\"\n",
    "    Build the file names for the output files for TimeSync\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'scsv': os.path.join(project_dir, f'prj_{project_id}/{plot_id}_spectral_files_set_{year_doy}.csv'),\n",
    "        'tcap': os.path.join(project_dir, f'prj_{project_id}/tc/plot_{plot_id}/plot_{plot_id}_{year_doy}.png'),\n",
    "        'b743': os.path.join(project_dir, f'prj_{project_id}/b743/plot_{plot_id}/plot_{plot_id}_{year_doy}.png'),\n",
    "        'b432': os.path.join(project_dir, f'prj_{project_id}/b432/plot_{plot_id}/plot_{plot_id}_{year_doy}.png')}\n",
    "\n",
    "\n",
    "def report_status(func: Callable) -> Callable:\n",
    "    @wraps(func)\n",
    "    def wrapper(plot: Tuple[Any, ...], *args, **kwargs) -> Tuple[Tuple[Any, ...], str]:\n",
    "        \"\"\"\n",
    "        Return the plot and any exception raised, or report complete\n",
    "        \"\"\"\n",
    "        try:\n",
    "            func(plot, *args, **kwargs)\n",
    "            return plot, 'complete'\n",
    "        except Exception as error:\n",
    "            return plot, str(error)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@report_status\n",
    "def process_plot(plot: Tuple[Any, ...], params: dict) -> None:\n",
    "    \"\"\"\n",
    "    Process an individual plot\n",
    "    \"\"\"\n",
    "    groups = group_records(stac_records_for_plot(plot, params))\n",
    "    for group in groups:\n",
    "        process_group(group, plot, params)\n",
    "\n",
    "\n",
    "@report_status\n",
    "def process_plot_dask(plot: Tuple[Any, ...], params: dict) -> None:\n",
    "    \"\"\"\n",
    "    Process an individual plot\n",
    "    \"\"\"\n",
    "    groups = group_records(stac_records_for_plot(plot, params))\n",
    "    func = partial(process_group, plot=plot, params=params)\n",
    "    with worker_client() as client:\n",
    "        futures = client.map(func, groups)\n",
    "        try:\n",
    "            client.gather(futures)\n",
    "        except Exception:\n",
    "            client.cancel(futures)\n",
    "            raise\n",
    "\n",
    "\n",
    "def check_bit(value: int, bit: int) -> bool:\n",
    "    \"\"\"\n",
    "    Check whether a bit is set\n",
    "    \"\"\"\n",
    "    return bool((value & (1 << bit)))\n",
    "\n",
    "\n",
    "def passes_qa_check(qa: int, enable_cloud_filtering=False) -> bool:\n",
    "    \"\"\"\n",
    "    Make sure the QA value is not indicating fill and (optionally) ensure clear or water bits are set\n",
    "    \"\"\"\n",
    "    if check_bit(qa, QA_FILL):\n",
    "        return False\n",
    "    if enable_cloud_filtering and not (check_bit(qa, QA_CLEAR) or check_bit(qa, QA_WATER)):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7290faa1-9fbc-483e-885d-f0fbbdfa0634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(fs, file: str) -> None:\n",
    "    \"\"\"\n",
    "    Create parent directories if it makes sense to do so\n",
    "    \"\"\"\n",
    "    if isinstance(fs, LocalFileSystem):\n",
    "        fs.makedirs(os.path.dirname(file), exist_ok=True)\n",
    "\n",
    "\n",
    "def stac_records_for_plot(plot: Tuple[Any, ...], params: dict) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Retrieve the stac records relevant for the plot\n",
    "    \"\"\"\n",
    "    query_results = []\n",
    "    roi = centered_bounds(plot.x, plot.y, params['chip_size'][0], params['chip_size'][1])\n",
    "    for h, v in determine_hvs(roi, region=params['region']):\n",
    "        query_results.extend(query_stac(build_query(h, v, region=params['region'])))\n",
    "    return query_results\n",
    "\n",
    "\n",
    "def format_plot_data(plot_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read in the csv file containing geospatial plot data\n",
    "    \"\"\"\n",
    "    return pd.read_csv(\n",
    "        plot_file,\n",
    "        usecols=['project_id', 'plot_id', 'x', 'y'],\n",
    "        dtype={'project_id': str, 'plot_id': str, 'x': int, 'y': int})\n",
    "\n",
    "\n",
    "def format_log_data(log_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read in the csv file containing a record of previous run(s)\n",
    "    \"\"\"\n",
    "    return pd.read_csv(\n",
    "        log_file,\n",
    "        usecols=['project_id', 'plot_id', 'time', 'status'],\n",
    "        dtype={'project_id': str, 'plot_id': str, 'time': str, 'status': str})\n",
    "\n",
    "\n",
    "def aws_credentials(profile: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Fetch information on AWS credentials\n",
    "    \"\"\"\n",
    "    # parser = configparser.ConfigParser()\n",
    "    # parser.read(os.path.join(os.environ['HOME'], '.aws', 'credentials'))\n",
    "    # return parser[profile]['aws_access_key_id'], parser[profile]['aws_secret_access_key']\n",
    "    return \n",
    "\n",
    "\n",
    "def local_setup(*args) -> dict:\n",
    "    \"\"\"\n",
    "    Extra setup for writing output to a local file system\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'fs': fsspec.filesystem('file'),\n",
    "        'rio_env': {\n",
    "            'session': None, \n",
    "            'GDAL_PAM_ENABLED': 'NO',  # Set to 'YES' to write XML metadata\n",
    "        }}\n",
    "\n",
    "\n",
    "def aws_setup(profile: str, *args) -> dict:\n",
    "    \"\"\"\n",
    "    Extra setup for writing to an S3 bucket\n",
    "    \"\"\"\n",
    "    # key, secret = aws_credentials(profile)\n",
    "    return {\n",
    "        # 'fs': fsspec.filesystem('s3', key=key, secret=secret),\n",
    "        'fs': fsspec.filesystem('s3', anon=False, requester_pays=True),\n",
    "        'rio_env': {\n",
    "            'session': rio.session.AWSSession(),\n",
    "            'GDAL_DISABLE_READDIR_ON_OPEN': 'EMPTY_DIR',\n",
    "            'GDAL_PAM_ENABLED': 'NO',  # Set to 'YES' to write XML metadata\n",
    "        }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347a5b04-da23-4a94-97fe-265a8a447bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(entry: list, csv_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Append a line to a csv file\n",
    "    \"\"\"\n",
    "    with open(csv_file, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(entry)\n",
    "\n",
    "\n",
    "def log_plot_status(plot: Tuple[Any, ...], status: str, log_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Write plot status to a log file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(log_file) or not (os.path.getsize(log_file) > 0):\n",
    "        append_to_csv(['project_id', 'plot_id', 'time', 'status'], log_file)\n",
    "    append_to_csv([plot.project_id, plot.plot_id, dt.now(), status], log_file)\n",
    "\n",
    "\n",
    "def data_preparation(plot_file: str, log_file: str) -> Tuple[pd.DataFrame, int, int]:\n",
    "    \"\"\"\n",
    "    Read in the plot geolocation information and prior processing history\n",
    "    \"\"\"\n",
    "    # Read in the plot data\n",
    "    plots_df = format_plot_data(plot_file)\n",
    "\n",
    "    if os.path.exists(log_file) and (os.path.getsize(log_file) > 0):\n",
    "        log_df = format_log_data(log_file)\n",
    "\n",
    "        # Get the most recent status from any previous processing run\n",
    "        df = plots_df.merge(\n",
    "            log_df.drop_duplicates(subset='plot_id', keep='last'),\n",
    "            how='left', on=['project_id', 'plot_id'])\n",
    "\n",
    "    else:\n",
    "        df = plots_df.copy().reindex(columns=plots_df.columns.tolist() + ['status'])\n",
    "\n",
    "    n_total, n_completed = len(df), len(df[df.status == 'complete'])\n",
    "    plots_to_process = df.loc[df.status != 'complete', plots_df.columns]\n",
    "\n",
    "    return plots_to_process, n_completed, n_total\n",
    "\n",
    "\n",
    "def submit_next_job(client: Client, func: Callable, plots: Iterable) -> Optional[Future]:\n",
    "    \"\"\"\n",
    "    Run the processing of the next plot as a job on the cluster\n",
    "    Set the priority of the job to favor completing plots in order\n",
    "    \"\"\"\n",
    "    try:\n",
    "        plot = next(plots)\n",
    "        future = client.submit(func, plot, priority=-int(plot.plot_id))\n",
    "    except StopIteration:\n",
    "        future = None\n",
    "    return future\n",
    "\n",
    "\n",
    "def log_file_name(params: dict) -> str:\n",
    "    \"\"\"\n",
    "    Define the output log file\n",
    "    \"\"\"\n",
    "    return os.path.splitext(params['plot_file'])[0] + '.log'\n",
    "\n",
    "\n",
    "def process_on_kube(params: dict) -> None:\n",
    "    \"\"\"\n",
    "    Process a group of plots on the CHS Pangeo Kubernetes cluster\n",
    "    \"\"\"\n",
    "    # Get input data\n",
    "    plots_df, n_completed, n_total = data_preparation(params['plot_file'], log_file_name(params))\n",
    "    if n_completed == n_total:\n",
    "        print(f'All {n_total} plots processed successfully! Exiting...')\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Get the client and dashboard link\n",
    "        client = params.pop('client')\n",
    "        print('Dashboard: https://pangeo.chs.usgs.gov' + client.dashboard_link)\n",
    "\n",
    "        # Define the processing function\n",
    "        processing_func = partial(process_plot_dask, params=params)\n",
    "\n",
    "        # Prepare to iterate over plots and hold resulting futures\n",
    "        plots, futures = plots_df.itertuples(), []\n",
    "\n",
    "        # Submit the first n tasks; others will be submitted as each task completes\n",
    "        for _ in range(CONCURRENT_STAC_QUERIES):\n",
    "            future = submit_next_job(client, processing_func, plots)\n",
    "            if future is not None:\n",
    "                futures.append(future)\n",
    "\n",
    "        # Prepare to process results as they complete\n",
    "        watch_for_completion = as_completed(futures)\n",
    "\n",
    "        # Track plot completion and handle subsequent task submissions\n",
    "        with tqdm.tqdm(desc='Processing plots', initial=n_completed, total=n_total) as pbar:\n",
    "            for completed in watch_for_completion:\n",
    "\n",
    "                # Log plot completion (or error message)\n",
    "                plot, status = completed.result()\n",
    "                log_plot_status(plot, status, log_file_name(params))\n",
    "                pbar.update()\n",
    "\n",
    "                # Submit the next plot to the cluster\n",
    "                future = submit_next_job(client, processing_func, plots)\n",
    "                if future is not None:\n",
    "                    watch_for_completion.add(future)\n",
    "\n",
    "\n",
    "def process_on_local(params: dict) -> None:\n",
    "    \"\"\"\n",
    "    Local single-threaded processing\n",
    "    \"\"\"\n",
    "    # Get input data\n",
    "    plots_df, n_completed, n_total = data_preparation(params['plot_file'], log_file_name(params))\n",
    "    if n_completed == n_total:\n",
    "        print(f'All {n_total} plots processed successfully! Exiting...')\n",
    "        return\n",
    "\n",
    "    # Define the processing function\n",
    "    processing_func = partial(process_plot, params=params)\n",
    "\n",
    "    # Iterate over the plots\n",
    "    for plot in tqdm.tqdm(plots_df.itertuples(), desc='Processing plots', initial=n_completed, total=n_total):\n",
    "        plot, status = processing_func(plot)\n",
    "        log_plot_status(plot, status, log_file_name(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c185a8e-7710-4b1a-852f-c70547fd5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timesync_data_extraction(project_dir: str, plot_file: str, region: str, chip_size: List[int], process_on: str,\n",
    "                             store_on: str, profile: Optional[str] = None, client: Optional[Client] = None) -> None:\n",
    "    \"\"\"\n",
    "    Run TimeSync data extraction\n",
    "    \"\"\"\n",
    "    params = locals()\n",
    "\n",
    "    storage = {\n",
    "        'local': local_setup,\n",
    "        'aws_s3': aws_setup,\n",
    "    }[store_on]\n",
    "\n",
    "    process = {\n",
    "        'kube_cluster': process_on_kube,\n",
    "        'local': process_on_local,\n",
    "    }[process_on]\n",
    "\n",
    "    params.update(storage(profile))\n",
    "    process(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545fe7fb-2cee-4445-877b-be933efb2719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing plots:   0%|                                 | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "timesync_data_extraction(**params)  # docker and the cluster will not need dask\n",
    "# timesync_data_extraction(**params, client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84513c63-93e7-47bb-b726-836441d33060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! head -11 TxL2Test_PlotList.csv >10lines_PlotList.csv\n",
    "! cat 10lines_PlotList.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063cdd0c-126d-4f16-9dd4-08618c3c7271",
   "metadata": {},
   "source": [
    "### 3. Start the cluster and get the client object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c222fa-e2b6-448c-bfca-dbc8b127d113",
   "metadata": {
    "tags": []
   },
   "source": [
    "See the three red rectangles on the left Jupyterlab sidebar? Click that button, and then at the bottom click on +NEW. At the bottom of the sidebar, you will see something like \"KubeCluster 1\". Click \"scale\" and choose some number of workers to utilize (max is 200), either by adaptive or manual scaling. The processing will go faster the more workers are assigned. Click the <> button at the bottom to get a code cell with the tcp address of the scheduler. It will appear above the cell you have currently highlighted. Click the cell below before clicking the button if you want it to appear under this text. Run that cell before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46e45a-75bc-42e9-8f03-508be3950ef5",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "Insert a cell above using the dask interface\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b6b813-872c-4159-89b2-58e75c749d67",
   "metadata": {},
   "source": [
    "### 4. Run the TimeSync extraction script (!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd91240-3a8d-4a45-a93c-8da25a998f57",
   "metadata": {},
   "source": [
    "At this point, the cluster and client should exist but do not yet have any tasks (there may or may not be workers; if using adaptive scaling, additional workers may appear only after the scheduler has received a sufficient number of tasks to keep them busy. Running the cell below will start submitting tasks to the scheduler. There are two types of tasks. The `process_plot_dask` task handles the processing of a single plot and launches large numbers of `process_group` tasks which write the actual data files for each observation. Additional `process_plot_dask` tasks will appear successively as plots are completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd332466-e250-4b4d-8bf5-52e2fb1939b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesync_data_extraction(**params, client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f20d8c0-766e-4b32-b7ff-9be3cae360c3",
   "metadata": {},
   "source": [
    "### If something goes wrong...\n",
    "\n",
    "Progress through the plots is tracked by a log file that is written to the same directory as the plot file (it will have the same name as the plot file and end in .log). If the script stops running or you lose the connection to the notebook, simply re-run all cells in the notebook (note: you may have to start a new cluster and retrieve a new client object) and the progress bar should come back to exactly where it left off. Alternatively, if you do want to start over completely, find the .log file in the aforementioned directory and delete/move/rename it. \n",
    "\n",
    "\n",
    "### On completion...\n",
    "\n",
    "Even if everything seems okay, re-run the cell above until it reports that all plots were processed successfully to ensure that no errors occurred. You should get a message like \"All n plots processed successfully! Exiting...\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
