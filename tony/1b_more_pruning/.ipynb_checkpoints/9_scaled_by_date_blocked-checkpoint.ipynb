{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa0a884-0bf0-4b06-8d0e-1c1764c800ff",
   "metadata": {},
   "source": [
    "### 1. Update the parameters dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f029d8-c363-4c44-bb91-64e00e0dab45",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "Update the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0aabbd-e8c9-40ac-b58b-fa7c1eefe17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_params = {\n",
    "    'project_dir': 's3://dev-nlcd-developer/junk3/timesync/', \n",
    "    'plot_file': './PlotList.csv', \n",
    "    'region': 'CU', \n",
    "    'chip_size': [255, 255]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296433e8-e714-4f50-9a5d-5c59e1223b70",
   "metadata": {},
   "source": [
    "### some of this AWS authentication stuff can be greatly simplified with %env or os.environment \n",
    "- like the requester pays bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca67668f-69e8-4ab4-8b50-4d2a70a82061",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env AWS_REQUEST_PAYER=requester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dabc800-3245-4f8c-b0e0-ab8dd8e58ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! aws s3 ls | grep dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91886244-db79-4452-aac1-8443c400fc7f",
   "metadata": {},
   "source": [
    "### 2. Import libraries and define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed80c1ba-0503-487e-a83d-0c0fb31797e1",
   "metadata": {},
   "source": [
    "Run the following cell, which contains all library imports and locally defined functions for data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9781419-c534-4383-85b4-9cdece0b3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import configparser\n",
    "from copy import copy\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime as dt\n",
    "from functools import partial, reduce, wraps\n",
    "from typing import List, Tuple, Optional, Any, Callable, Iterable\n",
    "\n",
    "import s3fs\n",
    "import tqdm\n",
    "import boto3\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pystac_client\n",
    "import rasterio as rio\n",
    "from dask.distributed import as_completed, worker_client, Client\n",
    "from dask.distributed.client import Future\n",
    "from fsspec.implementations.local import LocalFileSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e869dc-a5bb-434d-8ee9-e2dca161eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts_process_group import stac_records_for_plot, group_records\n",
    "from ts_process_group import process_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cacd24-cc93-4f5f-aa18-2d2d318df5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3538f6f6-79c7-4a88-a207-77ac3adf84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def report_status(func: Callable) -> Callable:\n",
    "    @wraps(func)\n",
    "    def wrapper(plot: Tuple[Any, ...], *args, **kwargs) -> Tuple[Tuple[Any, ...], str]:\n",
    "        \"\"\"\n",
    "        Return the plot and any exception raised, or report complete\n",
    "        \"\"\"\n",
    "        try:\n",
    "            func(plot, *args, **kwargs)\n",
    "            return plot, 'complete'\n",
    "        except Exception as error:\n",
    "            return plot, str(error)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "#@report_status\n",
    "def process_plot(plot: Tuple[Any, ...], params: dict) -> None:\n",
    "    \"\"\"\n",
    "    Process an individual plot\n",
    "    \"\"\"\n",
    "    print('called process_plot')\n",
    "    groups = group_records(stac_records_for_plot(plot, params))\n",
    "    for group in groups:\n",
    "        process_group(group, plot, params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_bit(value: int, bit: int) -> bool:\n",
    "    \"\"\"\n",
    "    Check whether a bit is set\n",
    "    \"\"\"\n",
    "    return bool((value & (1 << bit)))\n",
    "\n",
    "\n",
    "def passes_qa_check(qa: int, enable_cloud_filtering=False) -> bool:\n",
    "    \"\"\"\n",
    "    Make sure the QA value is not indicating fill and (optionally) ensure clear or water bits are set\n",
    "    \"\"\"\n",
    "    if check_bit(qa, QA_FILL):\n",
    "        return False\n",
    "    if enable_cloud_filtering and not (check_bit(qa, QA_CLEAR) or check_bit(qa, QA_WATER)):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a699b-eb36-4c90-aee7-708bec451cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def format_plot_data(plot_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read in the csv file containing geospatial plot data\n",
    "    \"\"\"\n",
    "    return pd.read_csv(\n",
    "        plot_file,\n",
    "        usecols=['project_id', 'plot_id', 'x', 'y'],\n",
    "        dtype={'project_id': str, 'plot_id': str, 'x': int, 'y': int})\n",
    "\n",
    "\n",
    "def format_log_data(log_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read in the csv file containing a record of previous run(s)\n",
    "    \"\"\"\n",
    "    return pd.read_csv(\n",
    "        log_file,\n",
    "        usecols=['project_id', 'plot_id', 'time', 'status'],\n",
    "        dtype={'project_id': str, 'plot_id': str, 'time': str, 'status': str})\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a5b04-da23-4a94-97fe-265a8a447bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(entry: list, csv_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Append a line to a csv file\n",
    "    \"\"\"\n",
    "    with open(csv_file, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(entry)\n",
    "\n",
    "\n",
    "def log_plot_status(plot: Tuple[Any, ...], status: str, log_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Write plot status to a log file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(log_file) or not (os.path.getsize(log_file) > 0):\n",
    "        append_to_csv(['project_id', 'plot_id', 'time', 'status'], log_file)\n",
    "    append_to_csv([plot.project_id, plot.plot_id, dt.now(), status], log_file)\n",
    "\n",
    "\n",
    "def data_preparation(plot_file: str, log_file: str) -> Tuple[pd.DataFrame, int, int]:\n",
    "    \"\"\"\n",
    "    Read in the plot geolocation information and prior processing history\n",
    "    \"\"\"\n",
    "    # Read in the plot data\n",
    "    plots_df = format_plot_data(plot_file)\n",
    "\n",
    "    if os.path.exists(log_file) and (os.path.getsize(log_file) > 0):\n",
    "        log_df = format_log_data(log_file)\n",
    "\n",
    "        # Get the most recent status from any previous processing run\n",
    "        df = plots_df.merge(\n",
    "            log_df.drop_duplicates(subset='plot_id', keep='last'),\n",
    "            how='left', on=['project_id', 'plot_id'])\n",
    "\n",
    "    else:\n",
    "        df = plots_df.copy().reindex(columns=plots_df.columns.tolist() + ['status'])\n",
    "\n",
    "    n_total, n_completed = len(df), len(df[df.status == 'complete'])\n",
    "    plots_to_process = df.loc[df.status != 'complete', plots_df.columns]\n",
    "\n",
    "    return plots_to_process, n_completed, n_total\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_file_name(params: dict) -> str:\n",
    "    \"\"\"\n",
    "    Define the output log file\n",
    "    \"\"\"\n",
    "    return os.path.splitext(params['plot_file'])[0] + '.log'\n",
    "\n",
    "\n",
    "def process_on_local(params: dict) -> None:\n",
    "    \"\"\"\n",
    "    Local single-threaded processing\n",
    "    \"\"\"\n",
    "    # Get input data\n",
    "    plots_df, n_completed, n_total = data_preparation(params['plot_file'], log_file_name(params))\n",
    "    print(plots_df)\n",
    "    if n_completed == n_total:\n",
    "        print(f'All {n_total} plots processed successfully! Exiting...')\n",
    "        return\n",
    "\n",
    "    # Define the processing function\n",
    "    processing_func = partial(process_plot, params=params)\n",
    "\n",
    "    for plot in plots_df.itertuples():\n",
    "        print(plot)\n",
    "        plot, status = processing_func(plot)\n",
    "\n",
    "    # Iterate over the plots\n",
    "    # for plot in tqdm.tqdm(plots_df.itertuples(), desc='Processing plots', initial=n_completed, total=n_total):\n",
    "    #     plot, status = processing_func(plot)\n",
    "    #     log_plot_status(plot, status, log_file_name(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6c134d-633c-4bf9-b5dc-65b9837f7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws_setup() -> dict:\n",
    "    \"\"\"\n",
    "    Extra setup for writing to an S3 bucket\n",
    "    \"\"\"\n",
    "    # key, secret = aws_credentials(profile)\n",
    "    return {\n",
    "        # 'fs': fsspec.filesystem('s3', key=key, secret=secret),\n",
    "        'fs': fsspec.filesystem('s3', anon=False, requester_pays=True),\n",
    "        'rio_env': {\n",
    "            'session': rio.session.AWSSession(),\n",
    "            'GDAL_DISABLE_READDIR_ON_OPEN': 'EMPTY_DIR',\n",
    "            'GDAL_PAM_ENABLED': 'NO',  # Set to 'YES' to write XML metadata\n",
    "        }}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c185a8e-7710-4b1a-852f-c70547fd5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timesync_data_extraction(project_dir: str, plot_file: str, region: str, chip_size: List[int]) -> None:\n",
    "    \"\"\"\n",
    "    Run TimeSync data extraction\n",
    "    \"\"\"\n",
    "    params = locals()\n",
    "\n",
    "    storage = {\n",
    "        'aws_s3': aws_setup,\n",
    "    }\n",
    "\n",
    "    # process = {\n",
    "    #     'local': process_on_local,\n",
    "    # }\n",
    "    print(storage)\n",
    "    params.update(storage)\n",
    "    # process(params)\n",
    "\n",
    "    process_on_local(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a7545-2bc7-4333-9071-32c9a332a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesync_data_extraction(**config_params)  # docker and the cluster will not need dask\n",
    "timesync_data_extraction(**params, client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84513c63-93e7-47bb-b726-836441d33060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! head -11 TxL2Test_PlotList.csv >10lines_PlotList.csv\n",
    "! cat PlotList.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d93c269-7b77-4a2b-895d-144b878f8039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
