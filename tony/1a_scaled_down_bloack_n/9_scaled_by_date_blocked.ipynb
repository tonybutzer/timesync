{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fa0a884-0bf0-4b06-8d0e-1c1764c800ff",
   "metadata": {},
   "source": [
    "### 1. Update the parameters dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f029d8-c363-4c44-bb91-64e00e0dab45",
   "metadata": {},
   "source": [
    "<div class=\"alert-warning\">\n",
    "Update the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0aabbd-e8c9-40ac-b58b-fa7c1eefe17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_params = {\n",
    "    'project_dir': 's3://dev-nlcd-developer/junk2/timesync/', \n",
    "    'plot_file': './PlotList.csv', \n",
    "    'region': 'CU', \n",
    "    'chip_size': [255, 255]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296433e8-e714-4f50-9a5d-5c59e1223b70",
   "metadata": {},
   "source": [
    "### some of this AWS authentication stuff can be greatly simplified with %env or os.environment \n",
    "- like the requester pays bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca67668f-69e8-4ab4-8b50-4d2a70a82061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: AWS_REQUEST_PAYER=requester\n"
     ]
    }
   ],
   "source": [
    "%env AWS_REQUEST_PAYER=requester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dabc800-3245-4f8c-b0e0-ab8dd8e58ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! aws s3 ls | grep dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91886244-db79-4452-aac1-8443c400fc7f",
   "metadata": {},
   "source": [
    "### 2. Import libraries and define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed80c1ba-0503-487e-a83d-0c0fb31797e1",
   "metadata": {},
   "source": [
    "Run the following cell, which contains all library imports and locally defined functions for data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9781419-c534-4383-85b4-9cdece0b3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import configparser\n",
    "from copy import copy\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime as dt\n",
    "from functools import partial, reduce, wraps\n",
    "from typing import List, Tuple, Optional, Any, Callable, Iterable\n",
    "\n",
    "import s3fs\n",
    "import tqdm\n",
    "import boto3\n",
    "import fsspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pystac_client\n",
    "import rasterio as rio\n",
    "from dask.distributed import as_completed, worker_client, Client\n",
    "from dask.distributed.client import Future\n",
    "from fsspec.implementations.local import LocalFileSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e869dc-a5bb-434d-8ee9-e2dca161eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts_stac_cog import Bounds\n",
    "from ts_stac_cog import StacRecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cacd24-cc93-4f5f-aa18-2d2d318df5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "458f80a1-2bbf-40ea-b57a-67656576ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "Affine = Tuple[float, float, float, float, float, float]\n",
    "\n",
    "\n",
    "# Constants\n",
    "QA_FILL = 0\n",
    "QA_CLEAR = 6\n",
    "QA_WATER = 7\n",
    "CONCURRENT_STAC_QUERIES = 2  # Prevents workers from consuming too much memory\n",
    "LANDSAT_ARD_C2_FILL_VALUE = 0\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def retry(retries: int, jitter: Tuple[int, int] = (1, 15)) -> Callable:\n",
    "    \"\"\"\n",
    "    Simple retry decorator, for retrying any function that may throw an exception\n",
    "    such as when trying to retrieve network resources\n",
    "    \"\"\"\n",
    "    def retry_dec(func: Callable) -> Callable:\n",
    "        def wrapper(*args, **kwargs):\n",
    "            count = 1\n",
    "            while True:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception:\n",
    "                    count += 1\n",
    "                    if count > retries:\n",
    "                        raise\n",
    "                    time.sleep(random.randint(*jitter))\n",
    "        return wrapper\n",
    "    return retry_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3538f6f6-79c7-4a88-a207-77ac3adf84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timesync_band_list() -> List[str]:\n",
    "    \"\"\"\n",
    "    Get the bands relevant to TimeSync data retrieval\n",
    "    \"\"\"\n",
    "    return ['blue', 'green', 'red', 'nir08', 'swir16', 'swir22', 'qa_pixel']\n",
    "\n",
    "\n",
    "def landsat_optical_band_list() -> List[str]:\n",
    "    \"\"\"\n",
    "    Get the optical wavelength Landsat bands\n",
    "    \"\"\"\n",
    "    return ['blue', 'green', 'red', 'nir08', 'swir16', 'swir22']\n",
    "\n",
    "\n",
    "def centered_window(x: float, y: float, width: int, height: int, ds: rio.io.DatasetReader) -> rio.windows.Window:\n",
    "    \"\"\"\n",
    "    Create a window centered on the x, y of a pixel of interest\n",
    "    Width and height are in pixels\n",
    "    \"\"\"\n",
    "    row_offset, col_offset = ds.index(x, y)\n",
    "    return rio.windows.Window(\n",
    "        col_offset - (width // 2),\n",
    "        row_offset - (height // 2),\n",
    "        width,\n",
    "        height)\n",
    "\n",
    "\n",
    "def centered_bounds(x: float, y: float, width: int, height: int, pixel_size: int = 30):\n",
    "    \"\"\"\n",
    "    Create coordinate bounds centered on an x/y coordinate\n",
    "    \"\"\"\n",
    "    return Bounds(\n",
    "        min_x=x - ((width // 2) * pixel_size),\n",
    "        max_x=x + ((width // 2) * pixel_size),\n",
    "        min_y=y - ((height // 2) * pixel_size),\n",
    "        max_y=y + ((height // 2) * pixel_size))\n",
    "\n",
    "\n",
    "def single_pixel_window(x: float, y: float, ds: rio.io.DatasetReader) -> rio.windows.Window:\n",
    "    \"\"\"\n",
    "    Get a window representing a single pixel\n",
    "    \"\"\"\n",
    "    row_offset, col_offset = ds.index(x, y)\n",
    "    return rio.windows.Window(col_offset, row_offset, 1, 1)\n",
    "\n",
    "\n",
    "def build_query(h: int, v: int, region: str = 'CU', collection: str = 'landsat-c2ard-sr',\n",
    "                datetime: str = '1984-01/1990-12-31', limit: Optional[int] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Construct a STAC query based on h/v tile coordinates\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'collections': collection,\n",
    "        'datetime': datetime,\n",
    "        'limit': limit,\n",
    "        'query': {'landsat:grid_horizontal': {'eq': f'{h:02}'},\n",
    "                  'landsat:grid_vertical': {'eq': f'{v:02}'},\n",
    "                  'landsat:grid_region': {'eq': region}}}\n",
    "\n",
    "\n",
    "def tile_grid_affine(region: str) -> Affine:\n",
    "    \"\"\"\n",
    "    Get the ARD tile grid affine based on the regional code\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'CU': (-2565585, 150000, 0, 3314805, 0, -150000),  # CONUS\n",
    "    }[region]\n",
    "\n",
    "\n",
    "def transform_geo(x: float, y: float, affine: Affine) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Perform the affine transformation from an x/y coordinate to row/col space.\n",
    "    \"\"\"\n",
    "    col = (x - affine[0] - affine[3] * affine[2]) / affine[1]\n",
    "    row = (y - affine[3] - affine[0] * affine[4]) / affine[5]\n",
    "    return int(col), int(row)\n",
    "\n",
    "\n",
    "def determine_hv(x: float, y: float, region: str) -> Tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Determine the ARD tile (in h/v coordinates) containing the x/y coordinate\n",
    "    \"\"\"\n",
    "    h, v = transform_geo(x, y, tile_grid_affine(region))\n",
    "    return h, v\n",
    "\n",
    "\n",
    "def determine_hvs(bbox, region: str) -> itertools.product:\n",
    "    \"\"\"\n",
    "    Determine the h/v coordinates of tiles that intersect a bounding box\n",
    "    \"\"\"\n",
    "    min_h, min_v = determine_hv(bbox.min_x, bbox.max_y, region)\n",
    "    max_h, max_v = determine_hv(bbox.max_x, bbox.min_y, region)\n",
    "    return itertools.product(range(min_h, max_h + 1), range(min_v, max_v + 1))\n",
    "\n",
    "\n",
    "def query_stac(query_params: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Query the STAC catalog using the provided query parameters\n",
    "    \"\"\"\n",
    "    stac = pystac_client.Client.open('https://landsatlook.usgs.gov/stac-server')\n",
    "    # This returns a dictionary with two keys, 'type' and 'features'\n",
    "    results = stac.search(**query_params).item_collection_as_dict()\n",
    "    # 'type' only contains the value 'FeatureCollection'; we care about what is in 'features'\n",
    "    return results['features']\n",
    "\n",
    "\n",
    "def group_dicts(records: List[dict], key_func: Callable) -> itertools.groupby:\n",
    "    \"\"\"\n",
    "    Group a list of dictionaries based on key value\n",
    "    \"\"\"\n",
    "    records = sorted(records, key=key_func)\n",
    "    return itertools.groupby(records, key=key_func)\n",
    "\n",
    "\n",
    "def convert_sr(data: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Re-scale Landsat Collection 2 spectral data values back to the Collection 1 range\n",
    "    \"\"\"\n",
    "    return ((data.astype(float) * 0.0000275 - 0.2) * 10000).astype(np.int16)\n",
    "\n",
    "\n",
    "def data_to_collection_1(old_dict: dict, bands: List[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Convert a dictionary of surface reflectance bands to the Landsat Collection 1 numerical range\n",
    "    \"\"\"\n",
    "    new_dict = old_dict.copy()\n",
    "    for key in bands:\n",
    "        new_dict[key] = convert_sr(old_dict[key])\n",
    "    return new_dict\n",
    "\n",
    "\n",
    "def read_bands(record: dict, bands: List[str], plot: Tuple[Any, ...], width: int, height: int) -> dict:\n",
    "    \"\"\"\n",
    "    Read in an ROI for an observation in the STAC record for all bands\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for band in bands:\n",
    "        with rio.open(StacRecord.asset_href(record, band)) as ds:\n",
    "            window = centered_window(plot.x, plot.y, width, height, ds)\n",
    "            # Get a masked array and fill it to avoid a bug with gdal/rasterio\n",
    "            out[band] = ds.read(1, window=window, boundless=True, fill_value=0, masked=True).filled()\n",
    "    return out\n",
    "\n",
    "\n",
    "def read_qa_at_plot(record: dict, plot: Tuple[Any, ...]) -> int:\n",
    "    \"\"\"\n",
    "    Read a single pixel at the plot location\n",
    "    \"\"\"\n",
    "    with rio.open(StacRecord.asset_href(record, 'qa_pixel')) as ds:\n",
    "        window = single_pixel_window(plot.x, plot.y, ds)\n",
    "        out = ds.read(1, window=window, boundless=False)\n",
    "    if out.size == 0:\n",
    "        return 1  # Treat values outside the spatial extent as fill\n",
    "    return out.item()\n",
    "\n",
    "\n",
    "def add_bands(dict_a: dict, dict_b: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Combine two observation dictionaries by adding the values for each band\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for band in dict_a:\n",
    "        out[band] = dict_a[band] + dict_b[band]\n",
    "    return out\n",
    "\n",
    "\n",
    "def composite(data: dict, bands: List[str], axis: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a multi-band ndarray from band names\n",
    "    \"\"\"\n",
    "    return np.stack([data[band] for band in bands], axis=axis)\n",
    "\n",
    "\n",
    "def tasseled_cap(data: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a composite of tasseled cap values\n",
    "    \"\"\"\n",
    "    band_order = ['blue', 'green', 'red', 'nir08', 'swir16', 'swir22']  # Must match coefficient order below\n",
    "    arr = np.stack([data[band] for band in band_order], axis=2)\n",
    "    b = np.tensordot(arr, [0.2043, 0.4158, 0.5524, 0.5741, 0.3124, 0.2303], axes=1)  # Brightness\n",
    "    g = np.tensordot(arr, [-0.1603, -0.2819, -0.4934, 0.7940, -0.0002, -0.1446], axes=1)  # Greenness\n",
    "    w = np.tensordot(arr, [0.0315, 0.2021, 0.3102, 0.1594, -0.6806, -0.6109], axes=1)  # Wetness\n",
    "    return np.stack([b, g, w])\n",
    "\n",
    "\n",
    "def build_affine(x_off: float, y_off: float, x_size: float = 30, y_size: float = 30, x_shear: float = 0,\n",
    "                 y_shear: float = 0) -> rio.Affine:\n",
    "    \"\"\"\n",
    "    Build the affine tuple in the rasterio format (different from GDAL)\n",
    "    \"\"\"\n",
    "    return rio.Affine(x_size, x_shear, x_off, y_shear, -y_size, y_off)\n",
    "\n",
    "\n",
    "def write_to_png(file: str, array: np.ndarray, crs: str, transform: rio.Affine) -> None:\n",
    "    \"\"\"\n",
    "    Write a PNG file as three 8-bit channels\n",
    "    \"\"\"\n",
    "    profile = {\n",
    "        'driver': 'PNG',\n",
    "        'count': 3,\n",
    "        'nodata': None,\n",
    "        'crs': crs,\n",
    "        'transform': transform,\n",
    "        'height': array.shape[1],\n",
    "        'width': array.shape[2],\n",
    "        'dtype': np.uint8}\n",
    "\n",
    "    with rio.open(file, mode='w', **profile) as ds:\n",
    "        ds.write(array)\n",
    "\n",
    "\n",
    "def array_mask(array: np.ndarray, value_to_mask = None, axis: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Boolean mask where the array matches the provided value anywhere along an axis\n",
    "    \"\"\"\n",
    "    return (array == value_to_mask).any(axis=axis)\n",
    "\n",
    "\n",
    "def apply_mask(array: np.ndarray, mask_array: np.ndarray, mask_value: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Apply a Boolean mask \n",
    "    \"\"\"\n",
    "    arr = array.copy()\n",
    "    arr[mask_array] = mask_value\n",
    "    return arr\n",
    "\n",
    "\n",
    "def byte_scale(array: np.ndarray, min_value: float, max_value: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Scale the data between min_value and max_value to 0-255\n",
    "    \"\"\"\n",
    "    out_array = (255 / (max_value - min_value)) * (array - min_value)\n",
    "    out_array = np.minimum(out_array, 255)\n",
    "    out_array = np.maximum(out_array, 0)\n",
    "    return out_array.astype(np.uint8)\n",
    "\n",
    "\n",
    "def byte_scale_bands(array: np.ndarray, all_bounds: List[Tuple[int, int]], \n",
    "                     mask: Optional[np.ndarray] = None, axis: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a multi-band array to scaled 8-bit\n",
    "    Masked values are set to 0\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for i, (min_value, max_value) in enumerate(all_bounds):\n",
    "        byte_image = byte_scale(array.take(i, axis=axis), min_value, max_value)\n",
    "        out.append(apply_mask(byte_image, mask, mask_value=0))\n",
    "    return np.stack(out, axis=axis)\n",
    "\n",
    "\n",
    "def center(array: np.ndarray) -> Tuple[int, ...]:\n",
    "    \"\"\"\n",
    "    Get the indices for the center of an array\n",
    "    \"\"\"\n",
    "    return tuple(x // 2 for x in array.shape)\n",
    "\n",
    "\n",
    "def center_value(array: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Get the center value of an array\n",
    "    \"\"\"\n",
    "    return array[center(array)]\n",
    "\n",
    "\n",
    "def spectral_data(data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Get the data for the center pixel in the chip\n",
    "    \"\"\"\n",
    "    return {band: center_value(array) for band, array in data.items()}\n",
    "\n",
    "\n",
    "def df_to_csv(df: pd.DataFrame, params: dict, output: dict) -> None:\n",
    "    \"\"\"\n",
    "    Write the dataframe to the csv file\n",
    "    \"\"\"\n",
    "    fs = fsspec.filesystem('s3', anon=False, requester_pays=True)\n",
    "\n",
    "    with fs.open(output['scsv'], 'w') as f:\n",
    "        df.to_csv(f, index=False)\n",
    "\n",
    "\n",
    "def classify_qa(qa: int) -> int:\n",
    "    \"\"\"\n",
    "    Return a value indicating the pixel is clear/water (0) or fill/cloud (1)\n",
    "    \"\"\"\n",
    "    if passes_qa_check(qa, enable_cloud_filtering=True):\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "def build_df(pixel_data: dict, record: dict, project_id: str, plot_id: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build the output dataframe\n",
    "    \"\"\"\n",
    "    return pd.DataFrame({\n",
    "        'sensor': StacRecord.sensor(record),\n",
    "        'project_id': project_id,\n",
    "        'plot_id': plot_id,\n",
    "        'hv': StacRecord.hv(record),\n",
    "        'year': StacRecord.year(record),\n",
    "        'doy': StacRecord.doy(record),\n",
    "        'blue': pixel_data['blue'],\n",
    "        'green': pixel_data['green'],\n",
    "        'red': pixel_data['red'],\n",
    "        'nir': pixel_data['nir08'],\n",
    "        'swir1': pixel_data['swir16'],\n",
    "        'swir2': pixel_data['swir22'],\n",
    "        'qa': classify_qa(pixel_data['qa_pixel']),\n",
    "        'date': StacRecord.date(record)}, index=[0])\n",
    "\n",
    "\n",
    "def invalid_pixel() -> dict:\n",
    "    \"\"\"\n",
    "    Get band values to represent and invalid pixel\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'blue': 0,\n",
    "        'green': 0,\n",
    "        'red': 0,\n",
    "        'nir08': 0,\n",
    "        'swir16': 0,\n",
    "        'swir22': 0,\n",
    "        'qa_pixel': 1}\n",
    "\n",
    "\n",
    "def adjust_for_s3(in_dict, filesystem) -> dict:\n",
    "    \"\"\"\n",
    "    Adjust the output file names to use the /vsis3/ file system handler for image data\n",
    "    \"\"\"\n",
    "    out_dict = copy(in_dict)\n",
    "    if isinstance(filesystem, s3fs.core.S3FileSystem):\n",
    "        for key in in_dict:\n",
    "            if in_dict[key].endswith('.png') or in_dict[key].endswith('.tif'):\n",
    "                out_dict[key] = in_dict[key].replace('s3:/', '/vsis3')\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "def process_group(group: List[dict], plot: Tuple[Any, ...], params: dict) -> None:\n",
    "    \"\"\"\n",
    "    Process a group of STAC records associated with a plot into output PNGs\n",
    "    \"\"\"\n",
    "    fs = fsspec.filesystem('s3', anon=False, requester_pays=True)\n",
    "    \n",
    "    # Set up the output filenames and (as applicable) directories\n",
    "    output = adjust_for_s3(\n",
    "        output_files(params['project_dir'], plot.project_id, plot.plot_id, StacRecord.year_doy(group[0])),\n",
    "        fs)\n",
    "    for out in output.values():\n",
    "        make_dirs(fs, out)\n",
    "\n",
    "    # Determine if the center pixel is fill\n",
    "    if not any(passes_qa_check(read_qa_at_plot(record, plot)) for record in group):\n",
    "        # Optionally, write an entry for an invalid pixel. This had been previous functionality,\n",
    "        # but because TimeSync does not expect this entry, I am disabling it.\n",
    "        # df_to_csv(build_df(invalid_pixel(), group[0], plot.project_id, plot.plot_id), params, output)\n",
    "        return\n",
    "\n",
    "    # Read and combine the data records\n",
    "    data_stack = [\n",
    "        read_bands(observation, timesync_band_list(), plot, params['chip_size'][0], params['chip_size'][1]) for\n",
    "        observation in group]\n",
    "    combined_data = data_to_collection_1(reduce(add_bands, data_stack), landsat_optical_band_list())\n",
    "\n",
    "   \n",
    "    # Get geospatial attributes\n",
    "    bounds = centered_bounds(plot.x, plot.y, params['chip_size'][0], params['chip_size'][1])\n",
    "    aff = build_affine(bounds.min_x, bounds.max_y)\n",
    "    crs = StacRecord.crs(group[0])\n",
    "    \n",
    "    # Mask for fill values\n",
    "    fill_value = convert_sr(np.array(LANDSAT_ARD_C2_FILL_VALUE))\n",
    "    mask = array_mask(composite(combined_data, landsat_optical_band_list()), fill_value)\n",
    "\n",
    "    # Calculate the output chip data\n",
    "    output_tcap = tasseled_cap(combined_data)\n",
    "    output_b743 = composite(combined_data, bands=['swir22', 'nir08', 'red'])\n",
    "    output_b432 = composite(combined_data, bands=['nir08', 'red', 'green'])\n",
    "\n",
    "    # Stretch the chip data and write to PNG\n",
    "    write_to_png(output['tcap'], byte_scale_bands(output_tcap, [(604, 5592), (49, 3147), (-2245, 843)], mask), crs, aff)\n",
    "    write_to_png(output['b743'], byte_scale_bands(output_b743, [(-904, 3696), (151, 4951), (-300, 2500)], mask), crs, aff)\n",
    "    write_to_png(output['b432'], byte_scale_bands(output_b432, [(151, 4951), (-300, 2500), (50, 1150)], mask), crs, aff)\n",
    "\n",
    "    # Define the metadata and spectral data for this observation and export to a csv\n",
    "    df_to_csv(build_df(spectral_data(combined_data), group[0], plot.project_id, plot.plot_id), params, output)\n",
    "\n",
    "\n",
    "def group_records(records: List[dict]) -> List[List[dict]]:\n",
    "    \"\"\"\n",
    "    Group records based on the observation ID\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for _, group in group_dicts(records, StacRecord.year_doy):\n",
    "        out.append(list(group))\n",
    "    return out\n",
    "\n",
    "\n",
    "def output_files(project_dir: str, project_id: str, plot_id: str, year_doy: str) -> dict:\n",
    "    \"\"\"\n",
    "    Build the file names for the output files for TimeSync\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'scsv': os.path.join(project_dir, f'prj_{project_id}/{plot_id}_spectral_files_set_{year_doy}.csv'),\n",
    "        'tcap': os.path.join(project_dir, f'prj_{project_id}/tc/plot_{plot_id}/plot_{plot_id}_{year_doy}.png'),\n",
    "        'b743': os.path.join(project_dir, f'prj_{project_id}/b743/plot_{plot_id}/plot_{plot_id}_{year_doy}.png'),\n",
    "        'b432': os.path.join(project_dir, f'prj_{project_id}/b432/plot_{plot_id}/plot_{plot_id}_{year_doy}.png')}\n",
    "\n",
    "\n",
    "def report_status(func: Callable) -> Callable:\n",
    "    @wraps(func)\n",
    "    def wrapper(plot: Tuple[Any, ...], *args, **kwargs) -> Tuple[Tuple[Any, ...], str]:\n",
    "        \"\"\"\n",
    "        Return the plot and any exception raised, or report complete\n",
    "        \"\"\"\n",
    "        try:\n",
    "            func(plot, *args, **kwargs)\n",
    "            return plot, 'complete'\n",
    "        except Exception as error:\n",
    "            return plot, str(error)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "#@report_status\n",
    "def process_plot(plot: Tuple[Any, ...], params: dict) -> None:\n",
    "    \"\"\"\n",
    "    Process an individual plot\n",
    "    \"\"\"\n",
    "    print('called process_plot')\n",
    "    groups = group_records(stac_records_for_plot(plot, params))\n",
    "    for group in groups:\n",
    "        process_group(group, plot, params)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_bit(value: int, bit: int) -> bool:\n",
    "    \"\"\"\n",
    "    Check whether a bit is set\n",
    "    \"\"\"\n",
    "    return bool((value & (1 << bit)))\n",
    "\n",
    "\n",
    "def passes_qa_check(qa: int, enable_cloud_filtering=False) -> bool:\n",
    "    \"\"\"\n",
    "    Make sure the QA value is not indicating fill and (optionally) ensure clear or water bits are set\n",
    "    \"\"\"\n",
    "    if check_bit(qa, QA_FILL):\n",
    "        return False\n",
    "    if enable_cloud_filtering and not (check_bit(qa, QA_CLEAR) or check_bit(qa, QA_WATER)):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c3a699b-eb36-4c90-aee7-708bec451cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dirs(fs, file: str) -> None:\n",
    "    \"\"\"\n",
    "    Create parent directories if it makes sense to do so\n",
    "    \"\"\"\n",
    "    if isinstance(fs, LocalFileSystem):\n",
    "        fs.makedirs(os.path.dirname(file), exist_ok=True)\n",
    "\n",
    "\n",
    "def stac_records_for_plot(plot: Tuple[Any, ...], params: dict) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Retrieve the stac records relevant for the plot\n",
    "    \"\"\"\n",
    "    query_results = []\n",
    "    roi = centered_bounds(plot.x, plot.y, params['chip_size'][0], params['chip_size'][1])\n",
    "    for h, v in determine_hvs(roi, region=params['region']):\n",
    "        query_results.extend(query_stac(build_query(h, v, region=params['region'])))\n",
    "    return query_results\n",
    "\n",
    "\n",
    "def format_plot_data(plot_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read in the csv file containing geospatial plot data\n",
    "    \"\"\"\n",
    "    return pd.read_csv(\n",
    "        plot_file,\n",
    "        usecols=['project_id', 'plot_id', 'x', 'y'],\n",
    "        dtype={'project_id': str, 'plot_id': str, 'x': int, 'y': int})\n",
    "\n",
    "\n",
    "def format_log_data(log_file: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read in the csv file containing a record of previous run(s)\n",
    "    \"\"\"\n",
    "    return pd.read_csv(\n",
    "        log_file,\n",
    "        usecols=['project_id', 'plot_id', 'time', 'status'],\n",
    "        dtype={'project_id': str, 'plot_id': str, 'time': str, 'status': str})\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "347a5b04-da23-4a94-97fe-265a8a447bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_to_csv(entry: list, csv_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Append a line to a csv file\n",
    "    \"\"\"\n",
    "    with open(csv_file, mode='a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(entry)\n",
    "\n",
    "\n",
    "def log_plot_status(plot: Tuple[Any, ...], status: str, log_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Write plot status to a log file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(log_file) or not (os.path.getsize(log_file) > 0):\n",
    "        append_to_csv(['project_id', 'plot_id', 'time', 'status'], log_file)\n",
    "    append_to_csv([plot.project_id, plot.plot_id, dt.now(), status], log_file)\n",
    "\n",
    "\n",
    "def data_preparation(plot_file: str, log_file: str) -> Tuple[pd.DataFrame, int, int]:\n",
    "    \"\"\"\n",
    "    Read in the plot geolocation information and prior processing history\n",
    "    \"\"\"\n",
    "    # Read in the plot data\n",
    "    plots_df = format_plot_data(plot_file)\n",
    "\n",
    "    if os.path.exists(log_file) and (os.path.getsize(log_file) > 0):\n",
    "        log_df = format_log_data(log_file)\n",
    "\n",
    "        # Get the most recent status from any previous processing run\n",
    "        df = plots_df.merge(\n",
    "            log_df.drop_duplicates(subset='plot_id', keep='last'),\n",
    "            how='left', on=['project_id', 'plot_id'])\n",
    "\n",
    "    else:\n",
    "        df = plots_df.copy().reindex(columns=plots_df.columns.tolist() + ['status'])\n",
    "\n",
    "    n_total, n_completed = len(df), len(df[df.status == 'complete'])\n",
    "    plots_to_process = df.loc[df.status != 'complete', plots_df.columns]\n",
    "\n",
    "    return plots_to_process, n_completed, n_total\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_file_name(params: dict) -> str:\n",
    "    \"\"\"\n",
    "    Define the output log file\n",
    "    \"\"\"\n",
    "    return os.path.splitext(params['plot_file'])[0] + '.log'\n",
    "\n",
    "\n",
    "def process_on_local(params: dict) -> None:\n",
    "    \"\"\"\n",
    "    Local single-threaded processing\n",
    "    \"\"\"\n",
    "    # Get input data\n",
    "    plots_df, n_completed, n_total = data_preparation(params['plot_file'], log_file_name(params))\n",
    "    print(plots_df)\n",
    "    if n_completed == n_total:\n",
    "        print(f'All {n_total} plots processed successfully! Exiting...')\n",
    "        return\n",
    "\n",
    "    # Define the processing function\n",
    "    processing_func = partial(process_plot, params=params)\n",
    "\n",
    "    for plot in plots_df.itertuples():\n",
    "        print(plot)\n",
    "        plot, status = processing_func(plot)\n",
    "\n",
    "    # Iterate over the plots\n",
    "    # for plot in tqdm.tqdm(plots_df.itertuples(), desc='Processing plots', initial=n_completed, total=n_total):\n",
    "    #     plot, status = processing_func(plot)\n",
    "    #     log_plot_status(plot, status, log_file_name(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f6c134d-633c-4bf9-b5dc-65b9837f7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aws_setup() -> dict:\n",
    "    \"\"\"\n",
    "    Extra setup for writing to an S3 bucket\n",
    "    \"\"\"\n",
    "    # key, secret = aws_credentials(profile)\n",
    "    return {\n",
    "        # 'fs': fsspec.filesystem('s3', key=key, secret=secret),\n",
    "        'fs': fsspec.filesystem('s3', anon=False, requester_pays=True),\n",
    "        'rio_env': {\n",
    "            'session': rio.session.AWSSession(),\n",
    "            'GDAL_DISABLE_READDIR_ON_OPEN': 'EMPTY_DIR',\n",
    "            'GDAL_PAM_ENABLED': 'NO',  # Set to 'YES' to write XML metadata\n",
    "        }}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c185a8e-7710-4b1a-852f-c70547fd5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timesync_data_extraction(project_dir: str, plot_file: str, region: str, chip_size: List[int]) -> None:\n",
    "    \"\"\"\n",
    "    Run TimeSync data extraction\n",
    "    \"\"\"\n",
    "    params = locals()\n",
    "\n",
    "    storage = {\n",
    "        'aws_s3': aws_setup,\n",
    "    }\n",
    "\n",
    "    # process = {\n",
    "    #     'local': process_on_local,\n",
    "    # }\n",
    "    print(storage)\n",
    "    params.update(storage)\n",
    "    # process(params)\n",
    "\n",
    "    process_on_local(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "545fe7fb-2cee-4445-877b-be933efb2719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'aws_s3': <function aws_setup at 0x7f4dfe91e520>}\n",
      "  project_id plot_id       x        y\n",
      "0       3132       1 -467100  1262760\n",
      "1       3132       2 -530580  1318380\n",
      "2       3132       3 -535680  1340610\n",
      "3       3132       4 -578520  1326270\n",
      "Pandas(Index=0, project_id='3132', plot_id='1', x=-467100, y=1262760)\n",
      "called process_plot\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtimesync_data_extraction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_params\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# docker and the cluster will not need dask\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# timesync_data_extraction(**params, client=client)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 18\u001b[0m, in \u001b[0;36mtimesync_data_extraction\u001b[0;34m(project_dir, plot_file, region, chip_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(storage)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# process(params)\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mprocess_on_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 69\u001b[0m, in \u001b[0;36mprocess_on_local\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m plot \u001b[38;5;129;01min\u001b[39;00m plots_df\u001b[38;5;241m.\u001b[39mitertuples():\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mprint\u001b[39m(plot)\n\u001b[0;32m---> 69\u001b[0m     plot, status \u001b[38;5;241m=\u001b[39m processing_func(plot)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "timesync_data_extraction(**config_params)  # docker and the cluster will not need dask\n",
    "# timesync_data_extraction(**params, client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84513c63-93e7-47bb-b726-836441d33060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! head -11 TxL2Test_PlotList.csv >10lines_PlotList.csv\n",
    "! cat 10lines_PlotList.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d93c269-7b77-4a2b-895d-144b878f8039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
