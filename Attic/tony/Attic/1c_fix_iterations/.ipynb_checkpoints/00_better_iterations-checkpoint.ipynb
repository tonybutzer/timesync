{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108ddf7a-5d3d-4b33-9bc3-917213d77521",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! aws s3 rm s3://dev-nlcd-developer/junk5/ --recur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca67668f-69e8-4ab4-8b50-4d2a70a82061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env AWS_REQUEST_PAYER=requester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc5e64f7-0c00-4714-b5fe-12902e66c9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python AWS and GDAL BS\n",
    "import os\n",
    "\n",
    "os.environ['AWS_REQUEST_PAYER'] = 'requester'\n",
    "os.environ['GDAL_DISABLE_READDIR_ON_OPEN']='EMPTY_DIR'\n",
    "os.environ['GDAL_HTTP_MAX_RETRY']='10'\n",
    "os.environ['GDAL_HTTP_RETRY_DELAY']='3'\n",
    "os.environ['GDAL_PAM_ENABLED']='NO'\n",
    "\n",
    "# 'GDAL_PAM_ENABLED': 'NO',  # Set to 'YES' to write XML metadata\n",
    "\n",
    "\n",
    " # GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR', \n",
    " #                 GDAL_HTTP_MAX_RETRY=10,\n",
    " #                 GDAL_HTTP_RETRY_DELAY=3):\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91886244-db79-4452-aac1-8443c400fc7f",
   "metadata": {},
   "source": [
    "### 2. Import libraries and define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed80c1ba-0503-487e-a83d-0c0fb31797e1",
   "metadata": {},
   "source": [
    "Run the following cell, which contains all library imports and locally defined functions for data extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9781419-c534-4383-85b4-9cdece0b3708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "# import time\n",
    "# import random\n",
    "# import itertools\n",
    "# import configparser\n",
    "# from copy import copy\n",
    "# from dataclasses import dataclass\n",
    "# from datetime import datetime as dt\n",
    "from functools import partial, reduce, wraps\n",
    "from typing import List, Tuple, Optional, Any, Callable, Iterable\n",
    "\n",
    "# import s3fs\n",
    "# import tqdm\n",
    "# import boto3\n",
    "# import fsspec\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "# import pystac_client\n",
    "# import rasterio as rio\n",
    "# from dask.distributed import as_completed, worker_client, Client\n",
    "# from dask.distributed.client import Future\n",
    "# from fsspec.implementations.local import LocalFileSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e869dc-a5bb-434d-8ee9-e2dca161eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ts_process_group import stac_records_for_plot, group_records\n",
    "from ts_process_group import process_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9cacd24-cc93-4f5f-aa18-2d2d318df5bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ts_log_stuff'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mts_log_stuff\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m format_plot_data\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ts_log_stuff'"
     ]
    }
   ],
   "source": [
    "from ts_log_stuff import format_plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3538f6f6-79c7-4a88-a207-77ac3adf84d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def report_status(func: Callable) -> Callable:\n",
    "#     @wraps(func)\n",
    "#     def wrapper(plot: Tuple[Any, ...], *args, **kwargs) -> Tuple[Tuple[Any, ...], str]:\n",
    "#         \"\"\"\n",
    "#         Return the plot and any exception raised, or report complete\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             func(plot, *args, **kwargs)\n",
    "#             return plot, 'complete'\n",
    "#         except Exception as error:\n",
    "#             return plot, str(error)\n",
    "\n",
    "#     return wrapper\n",
    "\n",
    "\n",
    "#@report_status\n",
    "def process_plot(plot: Tuple[Any, ...], params: dict) -> None:\n",
    "    \"\"\"\n",
    "    Process an individual plot\n",
    "    \"\"\"\n",
    "    print('called process_plot')\n",
    "    groups = group_records(stac_records_for_plot(plot, params))\n",
    "    for group in groups:\n",
    "        process_group(group, plot, params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a699b-eb36-4c90-aee7-708bec451cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f92b31-874e-462d-ac39-c98beef2d2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def data_preparation(plot_file: str, log_file: str) -> Tuple[pd.DataFrame, int, int]:\n",
    "    \"\"\"\n",
    "    Read in the plot geolocation information and prior processing history\n",
    "    \"\"\"\n",
    "    # Read in the plot data\n",
    "    plots_df = format_plot_data(plot_file)\n",
    "\n",
    "    if os.path.exists(log_file) and (os.path.getsize(log_file) > 0):\n",
    "        log_df = format_log_data(log_file)\n",
    "\n",
    "        # Get the most recent status from any previous processing run\n",
    "        df = plots_df.merge(\n",
    "            log_df.drop_duplicates(subset='plot_id', keep='last'),\n",
    "            how='left', on=['project_id', 'plot_id'])\n",
    "\n",
    "    else:\n",
    "        df = plots_df.copy().reindex(columns=plots_df.columns.tolist() + ['status'])\n",
    "\n",
    "    n_total, n_completed = len(df), len(df[df.status == 'complete'])\n",
    "    plots_to_process = df.loc[df.status != 'complete', plots_df.columns]\n",
    "\n",
    "    return plots_to_process, n_completed, n_total\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def log_file_name(plot_file):\n",
    "    \"\"\"\n",
    "    Define the output log file\n",
    "    \"\"\"\n",
    "    return os.path.splitext(plot_file)[0] + '.log'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aa861a-8402-4958-9a53-4cf703a09e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_on_local(project_dir, plot_file, region, chip_size):\n",
    "    \"\"\"\n",
    "    Local single-threaded processing\n",
    "    \"\"\"\n",
    "    # Get input data\n",
    "    plots_df, n_completed, n_total = data_preparation(plot_file, log_file_name(plot_file))\n",
    "    print(plots_df)\n",
    "    if n_completed == n_total:\n",
    "        print(f'All {n_total} plots processed successfully! Exiting...')\n",
    "        return\n",
    "        \n",
    "    params = {\n",
    "        'project_dir': project_dir, \n",
    "        'plot_file': plot_file, \n",
    "        'region': region, \n",
    "        'chip_size': chip_size\n",
    "    }\n",
    "    # Define the processing function\n",
    "    processing_func = partial(process_plot, params=params)\n",
    "\n",
    "    for plot in plots_df.itertuples():\n",
    "        print(plot)\n",
    "        plot, status = processing_func(plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c185a8e-7710-4b1a-852f-c70547fd5f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timesync_data_extraction(project_dir: str, plot_file: str, region: str, chip_size: List[int]) -> None:\n",
    "    \"\"\"\n",
    "    Run TimeSync data extraction\n",
    "    \"\"\"\n",
    "    # params = locals()\n",
    "    process_on_local(project_dir, plot_file, region, chip_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1f701f-05b4-4a2c-9da9-b7d0cee23f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_params = {\n",
    "    'project_dir': 's3://dev-nlcd-developer/junk4/timesync/', \n",
    "    'plot_file': './PlotList.csv', \n",
    "    'region': 'CU', \n",
    "    'chip_size': [255, 255]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0a7545-2bc7-4333-9071-32c9a332a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesync_data_extraction(**config_params)  # docker and the cluster will not need dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84513c63-93e7-47bb-b726-836441d33060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! head -11 TxL2Test_PlotList.csv >10lines_PlotList.csv\n",
    "! cat PlotList.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d93c269-7b77-4a2b-895d-144b878f8039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
